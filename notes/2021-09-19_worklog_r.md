OAK-D-IoT-40 – Luxonis
https://shop.luxonis.com/products/bw1092

OAK-D-IOT-40 — DepthAI Hardware Documentation 1.0.0 documentation
https://docs.luxonis.com/projects/hardware/en/latest/pages/DM1092.html#overview

First steps with DepthAI — DepthAI documentation | Luxonis
https://docs.luxonis.com/en/latest/pages/tutorials/first_steps/


https://github.com/luxonis/depthai
https://github.com/luxonis/depthai-python
https://github.com/luxonis/depthai-experiments

~~~
git clone https://github.com/luxonis/depthai 
cd depthai
python3 -m venv env
source env/bin/activate
pip install -U pip
python3 install_requirements.py
python3 depthai_demo.py -cam left
python3 depthai_demo.py -cam right
python3 depthai_demo.py -cam color
python3 depthai_demo.py -cnn face-detection-retail-0004
python3 depthai_demo.py -cnn deeplabv3p_person
python3 depthai_demo.py -cnn openpose2
~~~



~~~
(env) (base) user:~/ov/depthai$ python3 depthai_demo.py --help
Using depthai module from:  /home/user/ov/depthai/env/lib/python3.8/site-packages/depthai.cpython-38-x86_64-linux-gnu.so
Depthai version installed:  2.10.0.0.dev+7a0749a61597c086c5fd6e579618ae33accec8df
usage: depthai_demo.py [-h] [-cam {left,right,color}] [-vid VIDEO] [-dd] [-dnn] [-cnnp CNN_PATH] [-cnn CNN_MODEL] [-sh SHAVES] [-cnn_size CNN_INPUT_SIZE]
                       [-rgbr {1080,2160,3040}] [-rgbf RGB_FPS] [-dct DISPARITY_CONFIDENCE_THRESHOLD] [-lrct LRC_THRESHOLD] [-sig SIGMA] [-med {0,3,5,7}] [-lrc]
                       [-ext] [-sub] [-dff] [-scale SCALE [SCALE ...]]
                       [-cm {AUTUMN,BONE,CIVIDIS,COOL,DEEPGREEN,HOT,HSV,INFERNO,JET,MAGMA,OCEAN,PARULA,PINK,PLASMA,RAINBOW,SPRING,SUMMER,TURBO,TWILIGHT,TWILIGHT_SHIFTED,VIRIDIS,WINTER}]
                       [-maxd MAX_DEPTH] [-mind MIN_DEPTH] [-sbb] [-sbb_sf SBB_SCALE_FACTOR]
                       [-s {nn_input,color,left,right,depth,depth_raw,disparity,disparity_color,rectified_left,rectified_right} [{nn_input,color,left,right,depth,depth_raw,disparity,disparity_color,rectified_left,rectified_right} ...]]
                       [--report {temp,cpu,memory} [{temp,cpu,memory} ...]] [--report_file REPORT_FILE] [-sync] [-monor {400,720,800}] [-monof MONO_FPS]
                       [-cb CALLBACK] [--openvino_version {2020_3,2020_4,2021_1,2021_2,2021_3,2021_4}] [--count COUNT_LABEL] [-dev DEVICE_ID] [-bandw {auto,low,high}]
                       [-usbs {usb2,usb3}] [-enc ENCODE [ENCODE ...]] [-encout ENCODE_OUTPUT] [-xls XLINK_CHUNK_SIZE]
                       [-camo CAMERA_ORIENTATION [CAMERA_ORIENTATION ...]]

optional arguments:
  -h, --help            show this help message and exit
  -cam {left,right,color}, --camera {left,right,color}
                        Use one of DepthAI cameras for inference (conflicts with -vid)
  -vid VIDEO, --video VIDEO
                        Path to video file (or YouTube link) to be used for inference (conflicts with -cam)
  -dd, --disable_depth  Disable depth information
  -dnn, --disable_neural_network
                        Disable neural network inference
  -cnnp CNN_PATH, --cnn_path CNN_PATH
                        Path to cnn model directory to be run
  -cnn CNN_MODEL, --cnn_model CNN_MODEL
                        Cnn model to run on DepthAI
  -sh SHAVES, --shaves SHAVES
                        Number of MyriadX SHAVEs to use for neural network blob compilation
  -cnn_size CNN_INPUT_SIZE, --cnn_input_size CNN_INPUT_SIZE
                        Neural network input dimensions, in "WxH" format, e.g. "544x320"
  -rgbr {1080,2160,3040}, --rgb_resolution {1080,2160,3040}
                        RGB cam res height: (1920x)1080, (3840x)2160 or (4056x)3040. Default: 1080
  -rgbf RGB_FPS, --rgb_fps RGB_FPS
                        RGB cam fps: max 118.0 for H:1080, max 42.0 for H:2160. Default: 30.0
  -dct DISPARITY_CONFIDENCE_THRESHOLD, --disparity_confidence_threshold DISPARITY_CONFIDENCE_THRESHOLD
                        Disparity confidence threshold, used for depth measurement. Default: 245
  -lrct LRC_THRESHOLD, --lrc_threshold LRC_THRESHOLD
                        Left right check threshold, used for depth measurement. Default: 4
  -sig SIGMA, --sigma SIGMA
                        Sigma value for Bilateral Filter applied on depth. Default: 0
  -med {0,3,5,7}, --stereo_median_size {0,3,5,7}
                        Disparity / depth median filter kernel size (N x N) . 0 = filtering disabled. Default: 7
  -lrc, --stereo_lr_check
                        Enable stereo 'Left-Right check' feature.
  -ext, --extended_disparity
                        Enable stereo 'Extended Disparity' feature.
  -sub, --subpixel      Enable stereo 'Subpixel' feature.
  -dff, --disable_full_fov_nn
                        Disable full RGB FOV for NN, keeping the nn aspect ratio
  -scale SCALE [SCALE ...], --scale SCALE [SCALE ...]
                        Define which preview windows to scale (grow/shrink). If scale_factor is not provided, it will default to 0.5 
                        Format: preview_name or preview_name,scale_factor 
                        Example: -scale color 
                        Example: -scale color,0.7 right,2 left,2
  -cm {AUTUMN,BONE,CIVIDIS,COOL,DEEPGREEN,HOT,HSV,INFERNO,JET,MAGMA,OCEAN,PARULA,PINK,PLASMA,RAINBOW,SPRING,SUMMER,TURBO,TWILIGHT,TWILIGHT_SHIFTED,VIRIDIS,WINTER}, --color_map {AUTUMN,BONE,CIVIDIS,COOL,DEEPGREEN,HOT,HSV,INFERNO,JET,MAGMA,OCEAN,PARULA,PINK,PLASMA,RAINBOW,SPRING,SUMMER,TURBO,TWILIGHT,TWILIGHT_SHIFTED,VIRIDIS,WINTER}
                        Change color map used to apply colors to depth/disparity frames. Default: JET
  -maxd MAX_DEPTH, --max_depth MAX_DEPTH
                        Maximum depth distance for spatial coordinates in mm. Default: 10000
  -mind MIN_DEPTH, --min_depth MIN_DEPTH
                        Minimum depth distance for spatial coordinates in mm. Default: 100
  -sbb, --spatial_bounding_box
                        Display spatial bounding box (ROI) when displaying spatial information. The Z coordinate get's calculated from the ROI (average)
  -sbb_sf SBB_SCALE_FACTOR, --sbb_scale_factor SBB_SCALE_FACTOR
                        Spatial bounding box scale factor. Sometimes lower scale factor can give better depth (Z) result. Default: 0.3
  -s {nn_input,color,left,right,depth,depth_raw,disparity,disparity_color,rectified_left,rectified_right} [{nn_input,color,left,right,depth,depth_raw,disparity,disparity_color,rectified_left,rectified_right} ...], --show {nn_input,color,left,right,depth,depth_raw,disparity,disparity_color,rectified_left,rectified_right} [{nn_input,color,left,right,depth,depth_raw,disparity,disparity_color,rectified_left,rectified_right} ...]
                        Choose which previews to show. Default: []
  --report {temp,cpu,memory} [{temp,cpu,memory} ...]
                        Display device utilization data
  --report_file REPORT_FILE
                        Save report data to specified target file in CSV format
  -sync, --sync         Enable NN/camera synchronization. If enabled, camera source will be from the NN's passthrough attribute
  -monor {400,720,800}, --mono_resolution {400,720,800}
                        Mono cam res height: (1280x)720, (1280x)800 or (640x)400. Default: 400
  -monof MONO_FPS, --mono_fps MONO_FPS
                        Mono cam fps: max 60.0 for H:720 or H:800, max 120.0 for H:400. Default: 30.0
  -cb CALLBACK, --callback CALLBACK
                        Path to callbacks file to be used. Default: /home/user/ov/depthai/callbacks.py
  --openvino_version {2020_3,2020_4,2021_1,2021_2,2021_3,2021_4}
                        Specify which OpenVINO version to use in the pipeline
  --count COUNT_LABEL   Count and display the number of specified objects on the frame. You can enter either the name of the object or its label id (number).
  -dev DEVICE_ID, --device_id DEVICE_ID
                        DepthAI MX id of the device to connect to. Use the word 'list' to show all devices and exit.
  -bandw {auto,low,high}, --bandwidth {auto,low,high}
                        Force bandwidth mode. 
                        If set to "high", the output streams will stay uncompressed
                        If set to "low", the output streams will be MJPEG-encoded
                        If set to "auto" (default), the optimal bandwidth will be selected based on your connection type and speed
  -usbs {usb2,usb3}, --usb_speed {usb2,usb3}
                        Force USB communication speed. Default: usb3
  -enc ENCODE [ENCODE ...], --encode ENCODE [ENCODE ...]
                        Define which cameras to encode (record) 
                        Format: camera_name or camera_name,enc_fps 
                        Example: -enc left color 
                        Example: -enc color right,10 left,10
  -encout ENCODE_OUTPUT, --encode_output ENCODE_OUTPUT
                        Path to directory where to store encoded files. Default: /home/user/ov/depthai
  -xls XLINK_CHUNK_SIZE, --xlink_chunk_size XLINK_CHUNK_SIZE
                        Specify XLink chunk size
  -camo CAMERA_ORIENTATION [CAMERA_ORIENTATION ...], --camera_orientation CAMERA_ORIENTATION [CAMERA_ORIENTATION ...]
                        Define cameras orientation (available: AUTO, NORMAL, HORIZONTAL_MIRROR, VERTICAL_FLIP, ROTATE_180_DEG) 
                        Format: camera_name,camera_orientation 
                        Example: -camo color,ROTATE_180_DEG right,ROTATE_180_DEG left,ROTATE_180_DEG

~~~

## looking at depthai provided openpose2


https://docs.luxonis.com/en/latest/pages/tutorials/first_steps/#custom-handler

"The handler.py file should contain two methods - decode(nn_manager, packet) and draw(nn_manager, data, frames)"


ov/depthai/resources/nn/openpose2/handler.py
~~~
keypointsMapping = ['Nose', 'Neck', 'R-Sho', 'R-Elb', 'R-Wr', 'L-Sho', 'L-Elb', 'L-Wr', 'R-Hip', 'R-Knee', 'R-Ank',
                    'L-Hip', 'L-Knee', 'L-Ank', 'R-Eye', 'L-Eye', 'R-Ear', 'L-Ear']
POSE_PAIRS = [[1, 2], [1, 5], [2, 3], [3, 4], [5, 6], [6, 7], [1, 8], [8, 9], [9, 10], [1, 11], [11, 12], [12, 13],
              [1, 0], [0, 14], [14, 16], [0, 15], [15, 17], [2, 17], [5, 16]]
mapIdx = [[31, 32], [39, 40], [33, 34], [35, 36], [41, 42], [43, 44], [19, 20], [21, 22], [23, 24], [25, 26], [27, 28],
          [29, 30], [47, 48], [49, 50], [53, 54], [51, 52], [55, 56], [37, 38], [45, 46]]
colors = [[0, 100, 255], [0, 100, 255], [0, 255, 255], [0, 100, 255], [0, 255, 255], [0, 100, 255], [0, 255, 0],
          [255, 200, 100], [255, 0, 255], [0, 255, 0], [255, 200, 100], [255, 0, 255], [0, 0, 255], [255, 0, 0],
          [200, 200, 0], [255, 0, 0], [200, 200, 0], [0, 0, 0]]
~~~



call graph in the handler.py
~~~
-> decode()
- keypoints = getKeypoints(probMap, threshold)

- valid_pairs, invalid_pairs = getValidPairs(outputs, w, h, detected_keypoints)

- personwiseKeypoints = getPersonwiseKeypoints(valid_pairs, invalid_pairs, keypoints_list)
~~~




ov/depthai/resources/nn/openpose2/model.yml
~~~
description: >-
  2D human pose estimation from PINTO03009
documentation: https://github.com/PINTO0309/MobileNetV2-PoseEstimation/tree/master/models/train/test/openvino/mobilenet_v2_1.4_224/FP16
task_type: human_pose_estimation
files:
  - name: FP16/openpose2.xml
    size: 151699
    sha256: a8e6929e4b67472fe8086a05c4426d5f49af7e4383c9e9dfda8a5eae48f2529d
    source: https://raw.githubusercontent.com/PINTO0309/MobileNetV2-PoseEstimation/master/models/train/test/openvino/mobilenet_v2_1.4_224/FP16/frozen-model.xml
  - name: FP16/openpose2.bin
    size: 4409440
    sha256: 4f5d51729dc1cda4da7b402fe3e0af0c0858ac5f0288973623f8a747fa7a77f0
    source: https://github.com/PINTO0309/MobileNetV2-PoseEstimation/blob/master/models/train/test/openvino/mobilenet_v2_1.4_224/FP16/frozen-model.bin?raw=true
framework: dldt
license: https://github.com/PINTO0309/MobileNetV2-PoseEstimation/tree/master/models/train/test/openvino/mobilenet_v2_1.4_224/FP16
~~~



## looking at depthai pointed to blazepose 
This blazepose example Use Cases is pointed to by Luxonis

https://github.com/geaxgx/depthai_blazepose

~~~
git clone https://github.com/geaxgx/depthai_blazepose
cd depthai_blazepose

python3 -m venv env
source env/bin/activate
pip install -U pip
pip install -r requirements.txt

python3 demo.py

~~~

~~~
To facilitate reusability, the code is splitted in 2 classes:
    BlazeposeDepthai, which is responsible of computing the body landmarks. The importation of this class depends on the mode:
# For Host mode:
from BlazeposeDepthai import BlazeposeDepthai
# For Edge mode:
from BlazeposeDepthaiEdge import BlazeposeDepthai
    BlazeposeRenderer, which is responsible of rendering the landmarks and the skeleton on the video frame.
This way, you can replace the renderer from this repository and write and personalize your own renderer (for some projects, you may not even need a renderer).
~~~


See 
https://github.com/geaxgx/depthai_blazepose/tree/main/examples/semaphore_alphabet

See in Detail 
https://github.com/geaxgx/depthai_blazepose/blob/main/examples/semaphore_alphabet/demo.py


whole decoding done in recognize_gesture()

~~~
def recognize_gesture(b):  
    # b: body         

    def angle_with_y(v):
        # v: 2d vector (x,y)
        # Returns angle in degree of v with y-axis of image plane
        if v[1] == 0:
            return 90
        angle = atan2(v[0], v[1])
        return degrees(angle)

    # For the demo, we want to recognize the flag semaphore alphabet
    # For this task, we just need to measure the angles of both arms with vertical
    left_arm_angle = angle_with_y(b.landmarks[KEYPOINT_DICT['left_elbow'],:2] - b.landmarks[KEYPOINT_DICT['left_shoulder'],:2])
    right_pose = int((right_arm_angle +202.5) / 45) % 8 
    left_pose = int((left_arm_angle +202.5) / 45) % 8
    letter = semaphore_flag.get((right_pose, left_pose), None)
    return letter
~~~
